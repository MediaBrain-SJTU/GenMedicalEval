
# UniMedicalEval
我们提出了一个医疗大语言模型的综合评测框架，具有以下三大特点：

1.建立基础知识能力、临床应用能力、安全规范能力的多维度评价体系，提高评测全面性

2.建立、完善评测数据的构建流程，基于26,000+道医学考试题和55,000+份医学真实病历构建了总计100,000+条的高质量评测数据，提高评测可靠性

3.针对医学评测的特点设计面向生成式大语言模型的特色评价指标，提高评测区分性


## 1 基础知识能力
为了评测医疗大语言模型的基础知识能力，我们收集了涵盖xxxxx科室，从管培考试到执医考试层层递进全面、丰富......的医学考试题。
### 医学考试题分布








## 2 临床应用能力

为了评测医疗大语言模型在实际临床应用中的能力，我们收集了经过医疗专家验证和筛选的55,000例真实病例数据以构建与临床应用场景具有高度相关性的评测数据集。我们通过数据清洗、医生校验、场景划分、提问优化、调整格式等步骤将55,000例真实病例构建成涵盖六大场景二十种精细化医疗情境、数量总计超过80000例的大规模评测数据集，这使得UniMedicalEval能够在评估医疗模型的临床适用性和决策精度方面提供权威的参考标准。

## 2.1 数据集生成Pipeline

<p align="center">
  <img src=class/pipeline.png width=600px/>
</p>

- 数据收集：从网络上，医学考试题，医院的患者信息中收集原始数据。

- 数据清洗：对原始数据进行清洗和匿名化处理。数据清洗包括去掉患者病历中书写质量较差的，比较模板化回复缺乏实质信息，存在书写错误的病程记录等等。同时，我们还对医院中的化验单数据和影像学检查报告进行清洗。数据清洗的目的是确保清洗后的多种数据格式间有较为清晰的对应关系，尽量减少数据集中的噪声。
  
- 医生校验：专业医生从实用性，可靠性的角度对评估数据集进行校验。数据集校验的部分主要在于判断当前评估场景是否存在实际应用价值，是否是行之有效的医学问题以及是否与专业医生的逻辑思维关系相符合。专业医生校验的目的是为了让评估数据集能够符合人类医生的思维方式，有实际的应用价值和清晰的逻辑关系。

- 场景划分：通过与专业医生讨论，根据面向患者/面向医生对不同维度下的不同场景进行了划分，明确至六大维度20个精细化医疗场景，并对应的对评测数据集进行划分。
  
- 提问优化：由专业医生给出不同场景下需要思考与研究的问题，使评测数据集中的提问更专业与复杂。
  
- 调整格式：利用GPT-4将评测数据集构建成选择题与生成式格式便于评估。


## 2.2 场景划分与介绍

<p align="center">
  <img src=class/data4.png width=900px/>  
</p>


在临床应用能力评测方面，UniMedicalEval涵盖了6大医疗领域的任务和20种精细化医疗情境，具体场景划分如下所示，在每个细分医疗情境的超链接中我们给出了数据集示例和介绍。

<!--
|评测维度|	医疗场景|	数据量|	
|------|------------|-----------------|
|案例分析	|疾病诊断	|17,832	|
|案例分析|	诊断依据|	1,320	|
|案例分析|	医嘱建议	|6,420|	
|案例分析|	手术方案建议|	1,100	|
|案例分析|	风险咨询	|2,500	|
|知识问答|	疾病介绍	|2,000	|
|知识问答|	手术咨询|	800	|
|知识问答|	基础医疗问答|	12,000	|
|报告解读|	异常解读|	5,000	|
|报告解读|	化验项目选择|	5278	|
|报告解读|	报告诊断|	11,000|	
|报告解读|	检查推荐	|11,000	|
|报告解读|	治疗咨询	|8,000	|
|信息整合|	病历概要	|1,500	|
|信息整合|	术前信息整理|	1,500|	
|便捷问诊|	预问诊|	10,000	|
|便捷问诊|	快速问诊	|10,000	|
|情景对话|	在线问诊	|5,000|	
|情景对话|	医疗反事实|	10,000	|
|情景对话	|毒害伦理|	2,000	|
-->



**场景一：案例分析**

[疾病诊断](class/疾病诊断.md)，诊断依据，医嘱建议，手术方案建议，风险咨询

**场景二：知识问答**

疾病介绍，手术咨询，[基础医疗问答](class/基础医疗问答.md)

**场景三：报告解读**

异常解读，化验项目选择，[报告诊断](class/diseasecheck.md)，[检查推荐](class/检查推荐.md)，[治疗咨询](class/治疗咨询.md)

**场景四：信息整合**

病历概要，术前信息整理

**场景五：便捷问诊**

[预问诊](class/预问诊.md)，快速问诊

**场景六：情景对话**

在线问诊，[医疗反事实](class/医疗反事实.md)，毒害伦理


## 3.安全规范能力

为了评估医疗大语言模型的安全规范能力，我们从医疗反事实、毒害伦理、歧视校验、隐私保护等角度对模型进行评估。
根据《医疗质量与控制指标汇编》 (国家卫生健康委医政医管局指导编制)-人工智能辅助诊断技术临床应用质量控制指标(2017年版)，





## 4. 全面、特色、可靠的评测指标

为了对模型的基础知识能力、场景应用能力与安全规范能力进行全面、可靠的评估，UniMedicalEval设计了针对生成式大语言模型的多样化评测指标。
具体而言，UniMedicalEval的评测方式分为选择判断题（单选和多选）、生成式评测和大模型打分三种。三种评测方式具体如下：


## 4.1. 选择判断题

与开放式问题不同，选择题和判断题的答案存在于有限可数集合。在UniMedicalEval中，我们设计了严格的回复模板，只有大语言模型的回复精准匹配了这个模板才被评估为正确。
具体的评测prompt和回复模板如下（以单选题为示例）：

```
{ "Prompt": [n-shot demo, n is 0 for the zero-shot case],
    <User>：请基于病人的症状/化验报告单/检查报告/重要会诊结论回答以下问题，问题是：
    {题目}。从下列选项中选出唯一一个正确的答案：
    A. {选项A}
    B. {选项B}
    ...
    <Model>：正确答案是：
},
```


**评测数据与结论**

**🥇 Leaderboard**

更多结果请查看 [Leaderboard](class/leaderboard1.md).

| 模型| 疾病诊断 | 基础医疗问答| 报告诊断| 检查推荐| 治疗咨询| 预问诊 | 医疗反事实| 手术规划| 检查项目选择 |Avg|
|------|------------|-----------------|-----------------|------|------|------|------|------|------|------|
|GPT-4|0.79|0.72| 0.96|0.94|0.81|--|0.89|0.74|0.72|--|
|文心一言|0.63|0.60| 0.89|0.86|0.68|0.42|0.74|0.67|0.87|0.71|
|Huatuo2-13B|0.58|0.51| 0.96|0.94|0.65|--|0.57|--|--|--|
|通义千问|0.50|0.49|0.91|0.89|0.64|0.35|0.40|0.57|0.86|0.62|
|星火|0.45|0.47|0.87|0.80|0.57|0.40|0.39|--|--|--|
|Baichuan-chat-13B|0.27|0.24|0.63|0.32|0.29|--|0.48|--|--|--|
|InternLM-7B|0.25|0.22|0.15|0.07|0.18|--|0.47|--|--|--|

**实验结论**

-综合来看，GPT-4仍然是最强大的通用模型，但是在有的场景下表现会劣于Huatuo2-13B和文心一言

-文心一言和GPT-4达到了相近的性能，在大部分场景中都有不错的性能

-Huatuo2-13B是性能非常优异的领域小模型，超过了通义千问和星火大模型的表现，在部分场景下能达到与GPT-4接近的性能

-有的模型在处理如医疗反事实等场景时表现不佳，容易出现过于自信的现象而回答所有问题，无论问题本身是否可以回答。






## 4.2 生成式评测

由于大语言模型的回复往往是复杂的一般文本，并不遵从一种固定的格式。对于这种更一般的自然文本格式的大语言模型的回复，我们提出了一种基于结构化回复抽取和医学术语对齐的生成式评测方法来评测模型的能力。

此外，为了进行医学术语对齐，我们收集了[手术操作](https://github.com/MediaBrain-SJTU/UniCMedEval/blob/main/%E6%89%8B%E6%9C%AF%E6%93%8D%E4%BD%9C%E6%9C%AF%E8%AF%AD.xlsx)，[疾病诊断](https://github.com/MediaBrain-SJTU/UniCMedEval/blob/main/%E7%96%BE%E7%97%85%E8%AF%8A%E6%96%AD%E6%9C%AF%E8%AF%AD.xlsx)，[医疗操作](https://github.com/MediaBrain-SJTU/UniCMedEval/blob/main/%E5%8C%BB%E7%96%97%E6%93%8D%E4%BD%9C%E6%9C%AF%E8%AF%AD.xlsx)三个术语库，这三个术语库可以涵盖绝大多数包括疾病、治疗、检查在内的医学术语。

我们的生成式评测分为结构化回复抽取和术语对齐两个部分。将在下面详细介绍：

### 结构化回复抽取

结构化回复抽取指的是从模型的回复中提取出key-value格式的要点信息，将模型的回复从自然文本格式转变为字典格式。具体示例如下：

```
{ "original_output": "根据患者的症状、体征和检查结果，可以初步考虑以下疾病诊断和治疗建议：舌根新生物：患者主要症状是舌根处的新生物，可能需要进一步的检查以明确性质，例如切除组织并进行病理学检查。这有可能是良性病变，也可能是恶性肿瘤，所以需要进一步评估。慢性咽炎：患者有慢性咽炎的病史，这可能导致咽部不适和咳嗽咳痰。治疗建议包括喉咙保湿、避免刺激物质、遵医嘱使用抗炎药物或抗生素来缓解症状。Brugada波样改变：心电图显示Brugada波样改变，这可能与心脏的电传导异常有关。这需要进一步的心脏评估，包括心脏超声、心脏电生理检查等。治疗建议将根据进一步评估的结果而定，可能包括药物治疗或其他干预措施。",
    "structured_output": "疾病诊断：舌根新生物，慢性咽炎 治疗建议：喉咙保湿、使用抗炎药物或抗生素、心脏彩超",
    "Ground Truth": "疾病诊断：会厌囊肿 治疗建议：使用抗生素、心脏超声",
},
```
- original_output: 待测模型的原本输出
- structured_output：将待测模型的原本输出转化为结构化字典格式。此步骤通过在我们部分评测数据集上finetune的Baichuan-chat-13B完成。structured_output是key-value的字典格式，在上述例子中，key为"疾病诊断"和"治疗建议"，对应的value分别为"舌根新生物，慢性咽炎"和"喉咙保湿、使用抗炎药物或抗生素、心脏彩超"。
- Ground Truth：模型期望输出的答案。同样为key-value的字典格式，在上述例子中，key为"疾病诊断"和"治疗建议"，对应的value分别为"会厌囊肿"和"使用抗生素、心脏超声"。

### 医学术语对齐

由于在医学中存在同一术语的多种不同表达方式，为了更好的评测，我们通过医学术语对齐的方式来标准化模型回复和GT中的医学术语。具体流程如下：

<p align="center">
  <img src=class/术语对齐.png width=800px/>
</p>

- 1.基于structured_output，对于其每个key对应的value，利用分词得到多个单词。同理，对于GT也通过相同的方法，对每个key对应的value得到多个单词。
- 2.对1.中得到的单词和术语库中的所有单词提取Bert特征并计算相似度，将structured_output和GT中的每个单词都映射到术语库中最相似的单词上。
- 3.基于2.中映射后的结果计算生成式评测指标。


**评测数据与结论**

**🥇 Leaderboard**

对于生成式评测，我们定义了三种评测指标，分别为：

- Precision：结构化回复中字典的key与GT的key计算precision，表明大语言模型的回复中存在的逻辑点的准确率
- Recall：结构化回复中字典的key与GT的key计算Recall，表明大语言模型的回复中存在的逻辑点的召回率
- Bert score：结构化回复中字典的value与术语库对齐后与GT计算Bert score，表明大语言模型的回复中存在的知识点相对于答案的覆盖程度

更多结果请查看 [Leaderboard](class/leaderboard2.md).


## 4.3 大模型打分

一般来说，对于大语言模型的回复，可以从流畅性，完整性，科学性，相关性的方面利用GPT-4进行打分。但是使用GPT-4打分会导致潜在的数据泄露问题，为了避免这种数据泄露问题，UniMedicalEval基于指令微调训练了一个开源的评测专用的大语言模型。






## 🪶贡献

本项目由上海人工智能实验室、上海交通大学和华东师范大学合作完成。联合研发团队由王延峰教授领衔，成员包括[张娅](https://mediabrain.sjtu.edu.cn/yazhang/)教授、[王钰](https://mediabrain.sjtu.edu.cn/yuwang/)副教授、[王琳琳](https://faculty.ecnu.edu.cn/_s16/wll/main.psp)研究员，李然，贺樑，欧阳泽田，邱易帅，蔡琰，张燮驰，杨宇辰，廖育生，郭逸秋等。与本项目相关的论文有(https://arxiv.org/pdf/2309.02077v1.pdf)。


<!--
## 道德规范声明

本评测数据集使用的所有数据均已通过医院道德规范审查，保证了匿名性和安全性。
-->

## 引用

@misc{MedEvalHub,
  author={Yan Cai, Linlin Wang,Ye Wang, Gerard de Melo, Ya Zhang, Yan-Feng Wang, Liang He}, 
  title = {MedEvalHub: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models,
  year = {2024},
  publisher = {AAAI},
  journal = {Proceedings of Thirty-Eighth AAAI Conference on Artificial Intelligence(AAAI-2024)},
}
